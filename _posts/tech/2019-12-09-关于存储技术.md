---
layout: post
title: 存储技术
category: tech
tags: [storage,technology]
description: 低延迟的存储技术
---

# 背景

在进行大量的计算和存储的交互的时候，人们发现存储的延迟会大大的降低整个系统的性能。而现在的基于大数据以及大型计算的环境越来越多。所以在物理级别和软件层面都发力，用来突破短延时的情况下传输大量的数据。

# 技术层面

采用从内存到内存的RDMA方式来传递数据。
它需要的网卡共有三种技术格式。
分别是：
- iWARP
  - 使用TCP/IP以及以太网链路层来实现
- InfiniBand
  - 全套自有的硬件，包括IB的传输协议/网络层/链路层
- RoCE1/RoCE2
  - 使用IB的传输协议，但是跑在以太网链路层，尤其是RoCE2，使用udp进行包的封装和IP进行路由。
  

当前的网络带宽，以10Gpbs，25Gbps，40Gbps，50Gbps，100Gbps的比较普遍。一般都是使用叠加的方式。比如25Gbps，用3个10Gbps来实现，用1个25Gbps的网卡接受数据，然后后台用NVMe的存储卡来实现存储。

传输线材方面，有直连的双心铜线，但是缺点是距离比较短，3-7m，优点是成本低，延迟低。然后就是光纤线，一般传输距离100-200m之间。

铜线一般用于在同一个机柜内进行互联。光纤用于跨机柜和在数据中心内进行连接。


RoCE1在数据链路层上实现，缺点是只能在同一个广播域进行传输，不能路由。
RoCE2将global routing header封装在ip包头中，然后将UDP包头封装在
传输层进行转发。

有的网卡上内建一个eSwitch PCI-Express交换器，当几台要互联的时候，就直接使用端对端的互联即可。

<font color="red">iWARP与RoCE的差别</font>

- iWARP
  - 直接运行在以太网的网络层
  - 网络交换机不需要支持数据中心桥接功能

- Infiniband
  - 主要用在HPC环境中，支持到200GbE的频宽
  - 它也可以跑在以太网的网络层，但是必须使用infiniband自己的网络交换机，而不能是其他的类型的交换机


<b>RoCE的网卡供应商有mellanox，Qlogic，Emulex等</b>  
<b>iWARP的网卡供应商有chelsio，intel等</b>


# 存储模式

- single parity
  - 当S2D群集中有3台主机的时候，建议使用三项镜像的模式
  - 当只有2个主机的时候，那可以使用写一个副本的方式
  - 存储效率在66.7%-87.5%之间
- dual parity
  - 当群集中有4个主机的时候，可以建立，也就是写两份副本
  - 存储效率在50%-80%之间，随主机数量的增多而增多


# 微软的内存计算方式

假设每个微软的软件定义存储群集的主机配置了4个800GB的SSD存储，那么整体的存储为3.2TB，此时需要给该主机配置4*3.2=12.8GB的内存，用于提供元数据的存储和交换。

也就是说每1TB的存储，需要有4GB的内存来搭配。

同时采用NUMA架构的服务器更好，也就是每个cpu对应不同的存储，也就是平均分配存储给cpu，而不需要cpu跨NUMA节点获取存储，提高效率。


# 挑选SSD

SSD有资料读取寿命，即使简单的读也会影响。SSD固态盘在运行中还有个读取干扰的现象。随着存储密度不断增加，维持资料的可用性会降低。目前是通过缓冲区技术来保证可用性。

- 在消费领域，使用暂时性快取和电池来保证资料不会被遗失
- 在企业中，通过使用电容来保证不会因为失去电源而发生资料遗失

消费领域的规格：
- 1TB
- 读取一般在95000 IOPS
- 写入一般在90000 IOPS
- 资料可以读写185TB，保证5年

每日磁盘写入量（device writes per day，dwpd）

185TB /（365*5=1825 days）= 100GB 每天

也就是说如果每天写如100GB，那5年的寿命就结束。