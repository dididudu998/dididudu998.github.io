# 问题

以前一直用urllib库的方式来获取webpage的信息，也就是爬网，主要是不想看新闻，只想看内容，用这个可以get到内容，而且整理起来也方便。

然后用网上的库来收集一些别的地方的信息。

但是很多时候，这些地方都有针对爬网或者API的限制。怎么越过这个限制就是个问题，自己比较懒，其实也不聪明，想着为了越过这个东西要耗费不少时间，还不一定行，就放弃了。

但是人的想法有时候由于热情或者埋在心里的那种冲动会发生改变，为了看一些人发出来的信息，还是去做了。

# 过程

这个是针对短的信息的应用。利用API Key转为Application Key的方式来一次性获取更多的信息的方法。

- 获得access_token
  - 先获得API的custom key和custom secure key,到developer页面取注册id后，就可以获得了

  - 然后base64出来值，base64的格式是： customer key：custom secure key（这步现在可以省略了，postman的新版本支持了自动的变换，如果不能自动变化，那么post的headers里面就要卸乳Authorization的value为bearer base64_value）

    ```bash
    echo -n "customer key:custom secure key" |base64
    ```

  - 然后用到postman这个工具，将获得的base64编码的信息用这样的方式进行提交

    - POST的地址为：https://api.tw.com/oauth2/token
    - Authorization里面输入username为custom key，password为secure key
    - 在headers里面输入对应的值
      - User-Agent：my app v1.0
      - Content-Type：application/x-www-form-urlencoded
      - Accept-Encoding: application/gzip
    - 在body里面输入：grant_type=client_credentials

  - 然后send，如果都正确的话，会返回一个json的片段，里面包含有access_token。这个是我们想要获得的

- 获得想要关注的id的信息

  - 接下来还是用Postman，这个工具用起来真的很顺手。如果是小事情的话，比自己去用http的urlib这样的方式自己写要方便很多。但是如果是希望将来少动手，或者不动手的话，还是自己写比较好一点。我比较懒。

  - 这时候我们已经有了access_token，下来我们用get方法来获得信息

    - Get的地址为：https://api.tw.com/1.1/statuses/user_timeline.json?count=3000&screen_name=xinhuashe&page=9&User-Agent=my app v1.&Accept-Encoding=gzip

    - 这里要说明的是，问号后面的值都是在Params里面输入的。

      - count：3000，其实最多200
      - User-Agent：my app v1.
      - Accept-Encoding：gzip
      - screen_name: 这里要注意了，这个就是你要关注的人的账户名，不需要做任何的转换操作
      - page：9。 同样的，这个page是你可以修改的，如果他只有不到200个发文，那么page=1即可，如果他发文很多，那就page=2，3，4，5，就可以不断的获得他的发文信息。每一页200条。

    - 再下来就是Authorization部分了，在这里选择认证类型为Bearer Token，在token的文本框中输入我们获得的Access_token.

    - 然后单击send。如果网络都ok的话，那么很快就获得关注人的信息出现在Body的栏目里了。不过都是json格式的，包含有很多。类似下面这样的：

      ```json
      [
          {
              "created_at": "Mon May 12 16:37:55 +0000 2017",
              "id": 11330497057,
              "id_str": "1133041467648",
              "text": "如今https://t.co/bt5HNGg7xB",
              "truncated": false,
              "entities": {
                  "hashtags": [],
                  "symbols": [],
                  "user_mentions": [],
                  "urls": [],
                  "media": [
                      {
                          "id": 11328,
                          "id_str": "11486528",
                          "indices": [
                              26,
                              49
                          ],
                          "media_url": "http://pbs./D7lmr_ZVU.jpg",
                          "media_url_https": "https://pbs./D7lmr_ZVU.jpg",
                          "url": "https://t.co/bt7xB",
                          "display_url": "pic.com/bt7xB",
                          "expanded_url": "https://tPow/status/1133467648/photo/1",
                          "type": "photo",
                          "sizes": {
                              "thumb": {
                                  "w": 150,
                                  "h": 150,
                                  "resize": "crop"
                              },
                              "small": {
                                  "w": 680,
                                  "h": 626,
                                  "resize": "fit"
                              },
                              "large": {
                                  "w": 1370,
                                  "h": 1262,
                                  "resize": "fit"
                              },
                              "medium": {
                                  "w": 1200,
                                  "h": 1105,
                                  "resize": "fit"
                              }
                          }
                      }
                  ]
              },
      ```

    - 前面说过了，这个限定了一次只能取200条，那么就要对这获得的json文件进行清洗，我只想获得json文本中的text属性的值。

  - 清洗json文件，获得最终的信息

    - 开始想要golang来写的，但是发现格式转化老是为空，想着自己的能力还是太差。转而还是用python吧，直接点。
    - 我总共手动拷贝复制了16次，也就是获得了16个json文件。
    - 下面是python代码，反正就是粗暴直接：

    ```python
    import json
    tmp = []
    filename = []
    for num in range(1, 16):
        filename.append("file"+str(num)+".txt")
    for file in filename:
        with open(file) as f:
            data = json.load(f)
            for content in data:
                tmp.append(content['text'])
    with open("chunjing-pad.txt", "w") as f1:
        for line in tmp:
            f1.write(str(line)+"\n")
    ```

    - 获得了chunjing-pad.txt这个文本，里面就几千行吧，可以慢慢看了。不过网上也有很多直接获得图片和video的，目前还没这一个想法，就放弃了。



这个比直接用python的好的一点是，不会出现402这样的错误，402错误表示链接受限，可能是网络问题，SSL的问题，更大的可能是服务器端不允许链接了。而且速度很快，普通的发文人，顶多半小时就把他几年的发文给收下来了。

当然如果你想要持续的关注这个人，那么接下来就可以用stream的方式，实时进行监控。这里就不往下继续了。